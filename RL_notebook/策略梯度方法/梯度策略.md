![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214309.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214358.png)

怎么样得到这个式子呢？
首先是如果对rπbar求梯度的话，如果是discounted case 他是约等于 undiscounted case他是严格等于
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214552.png)

如果是vπbar他会严格等于
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214625.png)
如果是vπ0bar他会严格等于
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214654.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012214738.png)

这个就是stochastic gradient descent或者是ascent的基本的思路

那么怎么样得到这个式子呢?
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012215021.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012215132.png)

#### 补充说明
logπ要把π放到log里面那么π一定是要大于0的。
那么我们怎么确保所有的π对于所有的a都是大于0的呢？
我们可以用一个softmax function他可以normalize所有的属性在0 1之间
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012222116.png)
那么怎么选取这个h function呢？
我们可以用神经网络进行模拟。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012222254.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012222305.png)
如果action是无穷多个的话上一个方法就不行了，只能用deterministic的方法。