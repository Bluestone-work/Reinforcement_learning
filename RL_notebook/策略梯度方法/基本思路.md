policy gredient的方法是非常重要的。也是现在最流行的，相对来说性能最好的方法。
之前我们所用的方法都是value function approximation，之前是用表格形式表达，现在我们可以用函数（神经网络）的形式表达。
这节课和之前也比较相似，之前我们用表格来表示策略，现在我们用函数来表示策略。
之前的所有方法的是value-based，从这之后开始的方法都是policy-based
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012180021.png)

首先我们定义一个metric，这个metirc可以来定义什么样的策略是最优的。有了这个metric之后，我们就去做对应的优化，最简单的方法就是做梯度上升。
首先要计算出来metric的梯度是什么？之后我们就给出梯度上升的方法。
这时候就提出REINFORCE算法。

到目前位置我们的所有策略都是用表格来表示的。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012195748.png)
下面我们把表格改成函数，这时候π的写法就发生改变，和之前很类似，但是后面多了一个θ。
这个θ是一个向量，用来表示π函数里的参数。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012195847.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012200142.png)

表格和函数表示的区别是什么呢？
1. 我怎么样去定义一个最优策略呢？如果用表格来表示一个策略，一个策略最优的定义是所对应的Vπstar是最大的。如果是在函数情况下，我们会给与一个scalar metric一个标量的目标函数，然后我们去优化这个目标函数。![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012200433.png)
2. 怎么样去获取一个action他的probability，表格情况下，我想获取在s状态下take action a的概率，我直接查表就可以。用函数状态我们不能直接去索引了，我们得通过计算，比如说他是神经网络，我们得把s输入进去然他前向传播一次得到他的输出π(a1|s,θ)等等类似。![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012200827.png)
3. 我们怎样去updata一个策略？如果是表格的话，我们直接通过索引查表改变就好。如果是函数形式的表达就不能这样做了，因为这时候函数是用一个θ来表示的。只能去通过一个规则去改变他的θ，然后间接的去改变π(a|s)![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012201114.png)

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241012201337.png)
