![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240913220419.png)
### 最优策略定律
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240913220716.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240913220740.png)
 ![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920081238.png)
直观上来说action value就代表了action的价值。
首先对每个状态都选择action value最大的那个action，选择之后一直如此迭代，的到新的策略，最后就会得到一个最优的策略，这个过程用到的就是贝尔曼最优公式。
![](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920084630.png)
当我们加上这个max之后，这时候我们的Π就不再是累定的了，因为里面嵌套了一个优化问题，我们需要求解出这个Π，再将这个Π带入。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920084845.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920085041.png)
关于最优贝尔曼公式：
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920085304.png)
#### 试着求解贝尔曼最有公式
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920085519.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920085726.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240920085921.png)
