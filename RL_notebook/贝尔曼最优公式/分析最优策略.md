什么样的因素决定了最优策略呢？
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006110937.png)
我们要做的就是从这个式子中求解出这些黑色的变量，他们分别对应了最优的策略和最优的state value。已知的是这些红色的变量。所以最优策略就是由这些红色的变量所决定的。

用实际例子来考虑，对于同一个模型，如果我们设置的折扣率不同，那么决定的策略可能也不同。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006111306.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006111317.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006111452.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006111632.png)

当我们的每一个r都乘上了一个因子a + 上一个偏置，这时候整个系统的opyimal policy会改变吗？
举个例子，当我们一开始的
Rboundary = Rforbidden = -1，Rtarget = 1，
经过这个式子之后就会变成：
Rboundary = Rforbidden = 0 ， Rtarget = 2，Rother = 1，
相对而言并不会发生变化。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006112001.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006112210.png)
问题是第二幅图为什么不是最优策略。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006131245.png)
在大家设计reward的时候，每走一步就可以设置一个r = -1，类似于能量消耗，但是实际上是不是这样的，γ也是可以控制模型不走远路的，自然会找最短路径过去。
## 总结
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006131402.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241006131442.png)
