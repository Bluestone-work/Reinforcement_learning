## DPG Deterministic policy Gradient
DPG可以解决连续控制问题
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825180330.png)
自由度=2
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825180609.png)
策略网络是确定性的函数，策略网络也被称为actor。给定一个状态s输出的动作a事确定的，没有随机性，输出的动作a可以是实数，也可以是向量，这里的向量的维数是自由度(比如说机械手臂有两个关节，每个关节都要做出一个对应的动作) 他并不是说动作空间里只有两个动作。
价值网络也叫做critic，价值网络有两个输入，一个是状态s，另一个是动作a，基于状态s价值网络评价动作a的好坏程度。价值网络的输出是一个实数，这个实数是评估。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825180956.png)
首先使用TD算法更新价值网络。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181150.png)
at+1并不是实际在st+1时刻下做的动作，只是基于策略网络所做出的预测。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181324.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181500.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181510.png)
### improvment 
使用目标网络
回顾目标网络：
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181724.png)
TD算法鼓励TD target接近qt
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825181751.png)
假如一开始有低估，那么TDtarget就会有低估，低估会被传递回价值网络自身，导致低估会被一直传递回去。解决方案就是用目标网络计算TDtarget，尽量避免bootstrapping，这样性能稳定很多。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825182032.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825182200.png)
target network也有参数，也需要更新。
target network更新时也用到了value network 与policy network所以说是相关的。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825182339.png)

因此只能部分提升。
提升方式还有：
1.目标网络
2.经验回放
3.多步策略
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20240825182704.png)
