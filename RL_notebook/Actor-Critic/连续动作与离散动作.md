![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014220848.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014220938.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014221120.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014221222.png)
单步更新。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014221439.png)
总的来说，我们想要通过优化策略网络，使得评委打分尽可能的高。
如下两张图的网络优化其实是类似的。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014223555.png)
![](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014223652.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014223352.png)
DDPG主要是目标网络 target network + 经验回放 ReplayMemory
与Q网络优化不同的是，Q网络的Qtarget是不稳定的，因为是随机策略（预估的值），为了稳定这个Qtarget，DDPG分别给Q网络和策略网络都搭建了target_network，专门就是用来计算这个Q_target。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241014224056.png)
