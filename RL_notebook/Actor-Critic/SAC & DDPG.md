![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015154057.png)
第一类是由TRPO->PPO这一类：这些算法非常稳定，但不能拒绝前一次迭代的样本，因此其采样效率低。
第二类是有DDPG->TD3这一类：在这一类算法之下，你有确定性策略，非常重要的是，这些算法使用重播缓冲区，他们放了大量的样本，然后可以重复使用这些样本，但是他们非常的不稳定。

结合了第一类的随机策略和第二类的重播缓冲，实际上SAC并不是那么稳定，因为需要调优超参数以获得有效的性能，但它可能仍然比使用这种确定性算法更稳定。

在收集的每一个数据后做更多的梯度更新，这样可以允许神经网络从数据中去收集到更多的信息，从更少的数据中学习到更多，SAC和DDPG一般都有更好的样本复杂性。

#### DDPG
我们将从DDPG开始，然后谈到SAC，更进一步我们可以认为SAC就像是最大化熵版本的DDPG。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015161911.png)

这里的target可以用一个reward + gamma * Qst+1来表示，在A3C等等的一系列Generalized Advantage Estimation算法中都有着许多的变量，我们可以使用蒙特卡洛方法收集。在原始的DDPG论文中，他们使用了一步引导信号，自那以后已经做了很多变体，通常仍被称为DPG，在这些变体中你会使用多步奖励，之后使用Q函数作为你的目标。所以我们在进行Q学习，但我们是基于从当前策略或最近的策略中收集的数据。你可以查看我们遇到的每个状态下的Q函数，并且我们希望优化策略，这样如果我们在已经收集样本的状态下应用策略，那么Q函数将会预测到更高的价值。

它表示优化你的策略，以调整权重或动作，使其偏向具有高Q值的动作。顺便提一下，与之前的策略梯度方法不同标准策略梯度以及PPO和TRPO依赖于概率比策略梯度，这里的策略梯度通过Q函数如果你愿意的话，你的策略可以是一个确定性策略，当然，对于你的数据收集，你可能仍然希望有一些随机性，这就是为什么这里提到，可能加一些噪声，但你可以有一个确定性策略，这也是为什么它被称为深度确定性策略梯度（DDPG）。

它不一定是一个确定性策略，但如果你希望的话，它可以是确定性策略，然后你重复更多的演算，使用数据来进一步改进你的Q函数估计，更新你的策略，使其最大化回放缓冲区中的状态的Q值。

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015163631.png)


![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015163949.png)
DDPG的传统缺点是它可能有点不稳定，这就是Soft Actor-Critic的作用，它在很多地方已经成为首选方法，它通过在目标中加入熵来稳定系统，这将是一个最大熵公式，这将确保更好的探索，减少策略的过拟合，当然，需要确保熵不会衰减得太快，否则，你当然不会得到良好的探索。
Q函数在DDPG中偏向一个特定的动作，而策略可能会极力偏向这个动作，但Q函数可能仍然存在噪声，通过在目标中使用最大熵（max ENT），你的策略可能会更加分散，这样就不会在该特定动作上过于追求峰值，因为Q函数当前认为它是最好的。

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015164417.png)

就像我们在DDPG中看到的那样，优化你的策略以最大化Q值，你必须考虑到你现在实际上希望一个最大熵策略，这个策略优化Q值，这意味着一个有效的策略是Q值的指数化版本，在策略与Q值指数化编码策略之间的样本上通过最小化KL散度来实现，并重复直到收敛，Soft Actor Critic，当然，这些并不是完全优化好的，这是一个迭代优化过程。
我们将在这里进行一次或几次梯度更新，针对KL目标，仅进行几次梯度更新，然后会继续重复。

![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015164827.png)



![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015165026.png)
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241015165051.png)
DDPG（Deep Deterministic Policy Gradient）和SAC（Soft Actor-Critic）是两种常用于连续动作空间的强化学习算法。它们都属于**off-policy actor-critic**方法，利用了深度神经网络来表示策略和价值函数，但它们在策略优化和探索上的处理方式不同。接下来我将详细介绍这两个算法的核心思想、主要区别以及它们的优缺点。

### 1. DDPG（Deep Deterministic Policy Gradient）

#### 核心思想

DDPG 是由 **Deterministic Policy Gradient (DPG)** 算法发展而来的，将其与深度学习结合后形成了 DDPG。它适用于连续动作空间，主要结合了策略梯度方法与深度Q网络（DQN）的思想。

DDPG 是一个**确定性策略**（deterministic policy）的算法，其目标是直接输出给定状态下的动作，而不是动作的概率分布。以下是 DDPG 的主要组成部分：

- **Actor-Critic 架构**：
    
    - **Actor（策略网络）**：Actor 网络负责选择动作，输出连续动作的确定值。
    - **Critic（价值网络）**：Critic 网络估计状态动作值函数Q(s, a)，类似于 DQN 中的 Q 函数。
- **经验回放（Replay Buffer）**： DDPG 使用了经验回放技术，存储之前的状态、动作、奖励、下一状态四元组，从而打破数据相关性，使得算法更加稳定。
    
- **目标网络（Target Networks）**： DDPG 引入了两个目标网络（一个用于 Actor，一个用于 Critic），这些目标网络是主网络的延迟更新副本，能使学习过程更加平滑，避免发散。
    
- **策略更新**： 策略更新是基于策略梯度的，使用 Critic 网络估计出的Q(s,a)来指导 Actor 网络如何改进其选择的动作。
    
- **探索策略**： 为了进行探索，DDPG 在 Actor 网络输出的动作上加入噪声，常用的噪声类型是**Ornstein-Uhlenbeck 噪声**，这种噪声能产生具有时间相关性的探索行为。
#### 算法步骤：

- **Critic 更新**：通过最小化 TD 误差来更新 Critic 网络。
$$
L(\theta_Q) = \mathbb{E}_{s,a,r,s'}\left[\left(Q_{\theta}(s, a) - \left(r + \gamma Q_{\theta'}(s', \pi_{\theta'}(s'))\right)\right)^2\right]
$$
- **Actor 更新**：通过提升 Critic 提供的 Q 值来优化策略。
$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_s\left[ \nabla_a Q_{\theta}(s, a)\big|_{a=\pi_{\theta}(s)} \nabla_{\theta} \pi_{\theta}(s) \right]

$$
#### 优点：

- 适用于连续动作空间的任务。
- 能够很好地结合策略梯度和 Q-learning 的优点。
#### 缺点：
- 由于策略是确定性的，探索可能不足，导致收敛到局部最优解。
- 对于超参数敏感，且更新过程可能不稳定。
### 2. SAC（Soft Actor-Critic）

#### 核心思想

SAC 是一种**熵正则化的策略梯度算法**，其目标是在最大化期望回报的同时，增加策略的熵值。增加策略的熵鼓励更多的探索，从而提升算法的稳定性和泛化能力。

与 DDPG 不同，SAC 是一种**随机策略**（stochastic policy）算法，它输出动作的概率分布而非确定性动作。SAC 的目标不仅是最大化期望的回报，同时也最大化策略的熵值，以鼓励多样化的动作选择。

1. **Actor-Critic 架构**：
    
    - **Actor（策略网络）**：输出一个动作的概率分布，通常是高斯分布的参数（均值和方差），根据这个分布对动作进行采样。
    - **Critic（价值网络）**：估计状态动作值函数Q(s, a)，SAC 中通常会使用**两个 Critic 网络**，以减少高估误差。
2. **熵正则化**： SAC 在奖励函数中加入了熵项，目标是在最大化累积奖励的同时最大化策略的熵。这样可以鼓励策略的多样性，使得算法更加鲁棒。
$$
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t)}\left[r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]
$$
- 其中，H(π)\mathcal{H}(\pi)H(π) 表示策略的熵，α\alphaα 是一个权重系数，用于控制奖励与熵之间的平衡。
    
- **经验回放和目标网络**： 与 DDPG 相同，SAC 也使用经验回放池和目标网络进行稳定训练。
    
- **双 Q 网络（Double Q-Learning）**： SAC 使用两个 Q 网络来减小 Q 值的高估问题。在更新 Critic 网络时，选择两个 Q 值中较小的一个作为目标来更新。
    
- **自适应熵系数**： SAC 中的熵权重系数 α\alphaα 可以是固定的，也可以自适应调整，使得算法能根据不同环境自动调整探索的程度。
#### 算法步骤：

- **Critic 更新**：使用双 Q 网络的最小值进行 TD 目标更新。
$$
L(\theta_Q) = \mathbb{E}_{s,a,r,s'}\left[\left(Q_{\theta}(s, a) - \left(r + \gamma \left( \min_{i=1,2} Q_{\theta_i}(s', a') - \alpha \log \pi_{\theta}(a'|s') \right)\right)\right)^2\right]

$$
- **Actor 更新**：最大化 Q 值的同时增加策略的熵。
$$
J(\pi) = \mathbb{E}_s \left[\alpha \log \pi_{\theta}(a|s) - Q_{\theta}(s, a)\right]

$$
- **熵系数 α\alphaα 更新（可选）**：通过最小化目标熵与当前策略熵的差异来调整 α\alphaα。
$$
L(\alpha) = -\mathbb{E}_s\left[\alpha \left(\log \pi(a|s) + \mathcal{H}_{target}\right)\right]

$$
#### 优点：
- 高效的探索：通过熵正则化，SAC 可以鼓励策略的探索，使得算法收敛更加稳定，避免过早陷入局部最优。
- 双 Q 网络的使用减少了 Q 值高估问题，提升了学习的稳定性。
- 可自适应的熵系数使得 SAC 能自动调整探索和利用之间的平衡。
#### 缺点：
- SAC 由于使用了随机策略，计算代价比 DDPG 要高一些，因为在每一步中需要对动作进行采样。
- 训练时间较长，尤其在大规模问题上。
### 3. DDPG 和 SAC 的对比

|特性|DDPG|SAC|
|---|---|---|
|策略类型|确定性策略（Deterministic）|随机策略（Stochastic）|
|探索方式|添加噪声（通常是 Ornstein-Uhlenbeck）|熵正则化，鼓励随机性动作选择|
|Q 网络|单 Q 网络|双 Q 网络，减少 Q 值高估|
|目标|最大化累积奖励|最大化累积奖励 + 策略熵|
|学习稳定性|对超参数较为敏感，稳定性较差|学习稳定，探索效果好|
|训练时间|相对较快|训练时间较长，计算代价较高|
|适用场景|动作空间较小、探索需求较低的场景|动作空间大、需要高效探索的场景|

### 总结

- **DDPG** 适用于那些动作空间较为简单、确定性策略能够充分探索的任务。然而它容易陷入局部最优，且超参数选择对结果影响较大。
- **SAC** 则更为稳定和鲁棒，特别适合那些动作空间复杂、需要更多探索的环境。尽管计算开销较大，但 SAC 在复杂任务中的性能通常优于 DDPG。

两者的选择取决于具体任务的需求，以及对稳定性、计算效率、探索能力的平衡。

### Critic 1 (`critic_1`)

`critic_1` 的主要任务是评估当前的状态-动作对的价值（即Q值）。在训练过程中，它接收状态和动作的输入，通过网络输出一个标量，即当前状态和动作组合的 Q 值。这是该 Critic 的核心目标：通过最小化估计 Q 值和目标 Q 值之间的均方误差来提高预测的准确性。

在训练过程中：

1. **输入**：`state` 和 `action` 的拼接作为输入（输入维度为4），表示在某个状态下执行某个动作后的组合。
2. **损失函数**：损失函数为 Q 值的预测与目标值（由目标网络计算）之间的均方误差。代码中使用了 `torch.nn.functional.mse_loss` 来计算误差。
3. **目标值计算**：目标值是通过目标网络 `critic_1_target` 和 `critic_2_target` 来得到的最小 Q 值，减去下一个状态下动作的对数概率，结合奖励和折扣系数（0.99）来计算。

简而言之，`critic_1` 的优化目标是最小化当前 Q 值预测和目标 Q 值之间的差距，使其能够更准确地估计未来奖励。

### Critic 2 (`critic_2`)

`critic_2` 与 `critic_1` 的结构和功能相似，它同样是用来评估状态-动作对的 Q 值。然而，它的存在主要是为了引入双 Q 网络的机制，提升估值的稳定性。

双 Q 网络的优点是，通过使用两个独立的 Critic 网络，可以有效减少 Q 学习中的高估偏差。在更新 Q 值时，`critic_1` 和 `critic_2` 计算的值会取较小值（`torch.min(q_value_1, q_value_2)`），这种机制确保了在学习过程中更加保守，避免 Q 值被高估，从而提升学习的稳定性和性能。

在训练过程中：

1. **输入**：同样是状态和动作的组合。
2. **损失函数**：与 `critic_1` 一样，`critic_2` 也通过均方误差优化自己的 Q 值预测。
3. **目标值**：在计算目标 Q 值时，两个 Critic 网络计算出的目标 Q 值会取最小值作为最终目标。

### Critic 的共同优化过程

在训练过程中，`critic_1` 和 `critic_2` 都会计算状态-动作对的 Q 值，并且都通过反向传播来优化各自的网络权重。由于两个 Critic 网络结构相同且独立，因此每个 Critic 都有自己的优化器 (`optimizer_critic_1` 和 `optimizer_critic_2`) 来单独进行优化。

最后，在每一次迭代中，目标 Q 值是由两个 Critic 网络（`critic_1_target` 和 `critic_2_target`）计算并取最小值，这种机制可以降低过高估计 Q 值的风险，从而保证训练的稳定性和更可靠的价值估计。

`sample_action` 函数的作用是根据当前 `actor`（策略网络）的输出，生成一个动作（带有随机性），并计算该动作的对数概率（`log_prob`），用于后续的策略梯度更新。在强化学习中，策略的随机性和对数概率用于策略优化（比如在策略梯度方法中），而这个函数正是实现这一部分功能的核心。

以下是代码的详细解释：

### 1. `mean, std = actor(state)`

这一行通过调用 `actor` 网络，根据输入的 `state` 生成一个动作的均值 (`mean`) 和标准差 (`std`)。`actor` 网络的最后一层输出的是动作的均值和对数标准差，代码中通过 `exp` 对数标准差得到真正的标准差。

- `mean`: 动作的期望值（均值），是网络预测的确定性动作。
- `std`: 动作的标准差，表示动作生成过程中的随机性（用于探索）。

这一步让 `actor` 网络输出一个正态分布的参数（均值和标准差），从而可以在该分布中采样动作。

### 2. `normal_dist = torch.distributions.Normal(mean, std)`

这里使用 `torch.distributions.Normal` 来创建一个正态分布对象 `normal_dist`，该分布以 `mean` 为均值，以 `std` 为标准差。这个正态分布描述了在给定状态下，可能执行的动作的分布。

### 3. `action = normal_dist.rsample()`

这行代码从之前创建的正态分布中 **重参数化采样**（reparameterized sample）。`rsample()` 方法不同于 `sample()`，它允许在反向传播时能够计算出梯度，因此适合在强化学习中使用。这一步是根据 `mean` 和 `std`，为当前状态采样一个动作 `action`，并保留随机性。

重参数化技巧可以理解为：将随机性引入到采样过程中，同时确保采样结果的梯度能够正确计算，从而能够在策略优化时进行反向传播。

### 4. `log_prob = normal_dist.log_prob(action).sum(dim=-1, keepdim=True)`

这一行计算了所采样的动作的 **对数概率**。`log_prob` 是动作在分布 `normal_dist` 中的概率的对数值，它在后续优化中会用于更新策略。

- `normal_dist.log_prob(action)`: 计算动作 `action` 对应的对数概率。
- `.sum(dim=-1, keepdim=True)`: 由于可能有多个维度的动作（如果动作空间是多维的），对每个动作维度的对数概率求和，确保得到一个标量值。

在强化学习中，`log_prob` 是非常重要的，因为策略梯度算法（例如PPO、SAC等）中会用到动作的对数概率来更新策略。

### 5. `return action.tanh(), log_prob`

最终返回两个值：

- `action.tanh()`: 将采样得到的 `action` 通过 `tanh` 函数进行限制。`tanh` 函数将动作限制在区间 `(-1, 1)` 内，避免动作过大或者过小（尤其是对于连续动作空间来说，动作范围通常有一定的限制）。
- `log_prob`: 返回该动作在正态分布下的对数概率，用于后续的策略优化。

### 总结

- `sample_action` 函数的作用是在策略网络 `actor` 输出的正态分布上采样动作，并且计算该动作的对数概率。
- 该动作的采样带有随机性（源自正态分布），这是为了在策略优化中保持一定的探索性。
- 计算出的 `log_prob` 在后续优化时用于计算策略的梯度，反向传播调整策略，使得网络能够选择更好的动作。

这是强化学习中常见的**策略采样**过程，特别是在基于策略的算法中，比如策略梯度算法或SAC (Soft Actor-Critic) 等。