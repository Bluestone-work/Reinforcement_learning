![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241010205607.png)
为什么要用经验回放呢？且采样的时候为什么一定要服从均匀分布呢？
每个action怎么索引方式(S,A),这个就好像是二维平面上的一个点，虽然他是一个点但是他对应了两个坐标xy坐标是一样的，所以这里S和A他组成了一个随机变量，他就要服从这一个分布d。
R和S‘，当S和A给定之后，R和S’要服从系统的模型。
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241010205946.png)

数学上要求(S,A)是均匀分布，但是我采集数据的时候他一定是有先后顺序的，并且是按照其他的概率分布的，这时候不按照先后顺序进行使用就行了，用经验回放池里的数据，进行打散，然后从里面均匀的采样，这样就可以打破之前不同sample之间的correlation。这就是为什么经验回放是必须的。

tabular
为什么之前表格形式的Q_learning不需要经验回放呢？
因为之前没有涉及到状态(S,A)他的distribution，你只有要求他是按照一定distribution进行分布的时候比如说均匀分布，那我就需要把他打散然后再均匀采样。之前没有这个distribution的要求肯定就不需要这样做了。
为什么function形式的Q_learning就会涉及到（S，A）的分布呢？
这个问题的答案在于他们究竟在干嘛，deepQlearning是value function approximation的方法这一大类的方法都是在做optimization也就是他要有一个scaler的objective function然后去做优化，那么这个scaler objective function他是一个expectation，然后里面会涉及到很多东西，其中包括(S,A)所以你要求解这个目标函数里面肯定就要涉及到他的分布。
但是相反，我在基于表格的Q-learning或者TD算法的时候是在求解最优贝尔曼公式，就是对于每一个(S,A)都有一个组成这一组的式子，我们要把他求出来，这样就可以求出来最优的action value，所以他们在做的事情是不同的。
那么经验回放对于表格类的Qlearning能用吗？
是可以的。
之前的Q_learning他仿真需要10w步
![image.png](https://cdn.jsdelivr.net/gh/Bluestone-work/image/image/20241010214946.png)
