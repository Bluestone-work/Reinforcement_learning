{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Actor(\n",
       "   (fc): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "     (5): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " Critic(\n",
       "   (fc): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)  # Shape: [batch_size, num_actions]\n",
    "    \n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)  # Shape: [batch_size, num_actions]\n",
    "    \n",
    "# Initialize Models\n",
    "model_actor = Actor()\n",
    "model_critic = Critic()\n",
    "model_critic_delay = Critic()\n",
    "model_critic_delay.load_state_dict(model_critic.state_dict())\n",
    "model_actor , model_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Optimizers (Removed duplicate definitions)\n",
    "learning_rate_actor = 1e-3  # Adjusted learning rate for actor\n",
    "learning_rate_critic = 1e-2  # Adjusted learning rate for critic\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(model_actor.parameters(), lr=learning_rate_actor)\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=learning_rate_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def play(show = False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "    \n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        prob = model_actor(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "        a = random.choices(range(2), weights=prob, k=1)[0]\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "    state = np.array(state)\n",
    "    action = np.array(action)\n",
    "    reward = np.array(reward)\n",
    "    next_state = np.array(next_state)\n",
    "    over = np.array(over)\n",
    "    state = torch.FloatTensor(state).reshape(-1, 4)\n",
    "    action = torch.LongTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Function to Toggle Gradients\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def train_critic(state, reward, next_state, over):\n",
    "    requires_grad(model_actor, False)\n",
    "    requires_grad(model_critic, True)\n",
    "\n",
    "    #计算values和targets\n",
    "    value = model_critic(state)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target = model_critic_delay(next_state)\n",
    "    target = target * 0.99 * (1 - over) + reward\n",
    "\n",
    "    #时序差分误差,也就是tdloss\n",
    "    loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    optimizer_critic.zero_grad()\n",
    "\n",
    "    #减去value相当于去基线\n",
    "    return (target - value).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(state, action, value):\n",
    "    requires_grad(model_actor, True)\n",
    "    requires_grad(model_critic, False)\n",
    "\n",
    "    #重新计算动作的概率\n",
    "    prob = model_actor(state)\n",
    "    prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "    #根据策略梯度算法的导函数实现\n",
    "    #函数中的Q(state,action),这里使用critic模型估算\n",
    "    prob = (prob + 1e-8).log() * value\n",
    "    loss = -prob.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_actor.zero_grad()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -13.547459602355957 -975.4\n",
      "100 -7.610259056091309 -587.5\n",
      "200 -2.0842206478118896 -4.3\n",
      "300 -0.31977248191833496 -258.0\n",
      "400 -1.4571439027786255 149.15\n",
      "500 -0.9527031183242798 200.0\n",
      "600 -0.7511638402938843 200.0\n",
      "700 -0.47051215171813965 200.0\n",
      "800 -0.29454824328422546 200.0\n",
      "900 -0.5785565376281738 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_actor.train()\n",
    "    model_critic.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            value = train_critic(state, reward, next_state, over)\n",
    "            loss = train_actor(state, action, value)\n",
    "\n",
    "        #复制参数\n",
    "        for param, param_delay in zip(model_critic.parameters(),\n",
    "                                      model_critic_delay.parameters()):\n",
    "            value = param_delay.data * 0.7 + param.data * 0.3\n",
    "            param_delay.data.copy_(value)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
