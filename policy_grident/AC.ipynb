{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Actor(\n",
       "   (fc): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "     (5): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " Critic(\n",
       "   (fc): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)  # Shape: [batch_size, num_actions]\n",
    "    \n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)  # Shape: [batch_size, num_actions]\n",
    "    \n",
    "# Initialize Models\n",
    "model_actor = Actor()\n",
    "model_critic = Critic()\n",
    "model_critic_delay = Critic()\n",
    "model_critic_delay.load_state_dict(model_critic.state_dict())\n",
    "model_actor , model_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Optimizers (Removed duplicate definitions)\n",
    "learning_rate_actor = 1e-3  # Adjusted learning rate for actor\n",
    "learning_rate_critic = 1e-2  # Adjusted learning rate for critic\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(model_actor.parameters(), lr=learning_rate_actor)\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=learning_rate_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "def play(show = False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "    \n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        prob = model_actor(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "        a = random.choices(range(2), weights=prob, k=1)[0]\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 4)\n",
    "    action = torch.LongTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Function to Toggle Gradients\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define Function to Train the Critic\n",
    "def train_critic(states, actions, rewards, next_states, overs):\n",
    "    requires_grad(model_actor, False)\n",
    "    requires_grad(model_critic, True)\n",
    "\n",
    "    # Get current Q-values for the actions taken\n",
    "    current_q_values = model_critic(states).gather(dim=1, index=actions)  # Shape: [batch_size, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # For standard Q-Learning, use the max Q-value for the next state\n",
    "        next_q_values = model_critic_delay(next_states)  # Shape: [batch_size, num_actions]\n",
    "        max_next_q_values, _ = next_q_values.max(dim=1, keepdim=True)  # Shape: [batch_size, 1]\n",
    "        target_q_values = rewards + (1 - overs) * 0.99 * max_next_q_values  # Shape: [batch_size, 1]\n",
    "\n",
    "    # Compute Temporal Difference (TD) Loss\n",
    "    loss = F.mse_loss(current_q_values, target_q_values)\n",
    "\n",
    "    # Backpropagation and Optimization\n",
    "    optimizer_critic.zero_grad()  # Zero gradients before backward pass\n",
    "    loss.backward()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "    return current_q_values.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiangwang\\AppData\\Local\\Temp\\ipykernel_16068\\1864323377.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 4)\n"
     ]
    }
   ],
   "source": [
    "state, action, reward, next_state, over, reward_sum = play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(states, actions, q_values):\n",
    "    requires_grad(model_actor, True)\n",
    "    requires_grad(model_critic, False)\n",
    "    \n",
    "    # Forward pass to get action probabilities\n",
    "    prob = model_actor(states)  # Shape: [batch_size, num_actions]\n",
    "\n",
    "    # Select the probabilities of the actions taken\n",
    "    selected_prob = prob.gather(1, actions)  # Shape: [batch_size, 1]\n",
    "\n",
    "    # Compute log probabilities and multiply by Q-values\n",
    "    log_prob = torch.log(torch.clamp(selected_prob, min=1e-8)) * q_values  # Shape: [batch_size, 1]\n",
    "    loss = -log_prob.mean()  # Negative to perform gradient ascent\n",
    "\n",
    "    # Backpropagation and Optimization\n",
    "    optimizer_actor.zero_grad()  # Zero gradients before backward pass\n",
    "    loss.backward()\n",
    "    optimizer_actor.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1.2467421293258667 -981.75\n",
      "100 -69.16089630126953 -655.6\n",
      "200 -93.46170043945312 -215.4\n",
      "300 -0.9103409647941589 200.0\n",
      "400 38.72722244262695 200.0\n",
      "500 58.340972900390625 200.0\n",
      "600 75.28013610839844 147.4\n",
      "700 81.35139465332031 149.05\n",
      "800 86.90966033935547 96.4\n",
      "900 94.02978515625 149.55\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_actor.train()\n",
    "    model_critic.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            value = train_critic(state,action, reward, next_state, over)\n",
    "            loss = train_actor(state, action, value)\n",
    "\n",
    "        #复制参数\n",
    "        for param, param_delay in zip(model_critic.parameters(),\n",
    "                                      model_critic_delay.parameters()):\n",
    "            value = param_delay.data * 0.7 + param.data * 0.3\n",
    "            param_delay.data.copy_(value)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
